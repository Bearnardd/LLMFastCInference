 {
   "training": {
        "batch_size": 128,
        "max_seq_len": 256,
        "vocab_source": "llama2",
        "gradient_accumulation_steps": 4,
        "learning_rate": 5e-4,
        "max_iters": 100000,
        "weight_decay": 1e-1,
        "beta1": 0.9,
        "beta2": 0.95,
        "grad_clip": 1.0,
        "lr_decay_iter": 10000,
        "min_lr": 0.0,
        "decay_lr": true,
        "warmup_iters": 1000,
        "device": "cuda",
        "dtype": "float16",
        "compile": true,
        "output_dir": "out",
        "eval_interval": 2000,
        "log_interval": 1,
        "eval_iters": 100,
        "eval_only": true,
        "always_save_checkpoint": false

    },
    "model": {
        "type": "mistral",
        "dim": 288,
        "vocab_size": 32000,
        "hidden_dim": 50,
        "head_dim": 20,
        "max_batch_size": 2,
        "sliding_window": 3,
        "norm_eps": 0.01,
        "n_layers": 6,
        "n_heads": 6,
        "n_kv_heads": 6,
        "multiple_of": 32,
        "dropout": 0.0,
        "rotary_emb": "GPT-J",
        "output_layers": "rmsnorm->linear(145,320)->linear"
    }
}